{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e9499-2417-4202-a331-6df8a4d1f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from evaluate import load as load_metric\n",
    "import json\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import shutup\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(1234)\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d704f5-6b82-4459-9f25-ba93a479294f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57000d7c-1379-4b5a-b977-8ac1b94c5380",
   "metadata": {},
   "source": [
    "## OpenAI API (ChatGPT 4o mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8543f3-9261-47b6-8c49-4e670d38ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key='sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n",
    "\n",
    "def chatGPT(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    temperature=0.0\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00047e-ffe7-4f37-84ee-c2d61b93041a",
   "metadata": {},
   "source": [
    "## LOPSIDED Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd76f7-f90b-41c1-bfce-f06df62de186",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmodel = AutoModelForCausalLM.from_pretrained('./privacymodel/')\n",
    "rmodel = AutoModelForCausalLM.from_pretrained('./replacementmodel/')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./gemma-2-2b-it/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e417e5-3d92-4ec0-9b3c-7ea3324c0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy = pipeline('text-generation', model=pmodel, tokenizer=tokenizer, do_sample=True, temperature=0.1)replacement = pipeline('text-generation', model=rmodel, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e51881c-fbd1-4ed3-88a1-d165e619555f",
   "metadata": {},
   "source": [
    "## System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c8385-3b95-4a44-b474-ea2f6a9e2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_prompt = \"\"\"You are ReplaceGPT, an entity replacement model. Your task is to take an input, and output a transformed response that replaces all of the entities specified.\n",
    "\n",
    "The goal is to minimize the impact of changing the entities. The user should not be able to tell this transformation happened.\n",
    "\n",
    "The user will provide input of the original text, and a list of the entities that must be changed.\n",
    "\n",
    "You will output the modified text.\n",
    "\n",
    "Do not make any unnecesary changes that effect the semantic quality of the text, the meaning should stay the same.\n",
    "\n",
    "Only the entities themselves should change, not the meaning.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f28d96a-583a-4c5f-b2e2-de971c446bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_prompt = \"\"\"\n",
    "You are PrivacyGPT. You will anonymize the user's prompt while maintaining the meaning whenever possible.\n",
    "\n",
    "Your task is to revise the user's prompt. Your goal is to reword and change all private entities that are not strictly relevant to the text. You can change any names, places, organizations, etc as long as they wont effect the response when changed back.\n",
    "\n",
    "Remember the following criteria:\n",
    "* The meaning of the prompt **must not change**.\n",
    "* Any replacement of names, places, organizations, etc must be semantically similar to their original values. \n",
    "    * Don't use placeholders like [NAME]. Opt for similar entities, such as names with the same gender, organizations in the same field, etc.\n",
    "    * We will replace these replacements again when their response is completed, so the user does not notice the effect.\n",
    "* If the private entities are crucial to the meaning of the prompt then they must stay as they appear.\n",
    "    * For example, a location may remain in the prompt if it is absolutely needed to create a response and a replacement would not work.\n",
    "    * Works of literature often do not rely on the entity remaining the same, but there are exceptions, for example if the user requests a rhyming poem or song.\n",
    "* You are **maximizing the privacy** of the user, and **minimizing the effect on their request's reponse**.\n",
    "\n",
    "You will return your reasoning for each change alongside the change itself. At the end, provide the fully modified prompt as well as the original prompt.\n",
    "\n",
    "**REMEMBER: ONLY REPLACE THE WORD/TOKEN IF IT WILL NOT CHANGE THE ANSWER OR RESPONSE OF THE QUESTION OR TASK.**\n",
    "\n",
    "Here is the prompt:\n",
    "\n",
    "{prompt}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd8d2b-81fa-498a-9e02-5c766f3f2430",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af2d1b9-8e16-4fe7-8acf-a9f9b71ccca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reply(api_response):\n",
    "    return api_response['response']['body']['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ded94-4a22-4892-9f9b-6703582ead71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_changes(r):\n",
    "    try:\n",
    "        replacements =  [x.group()[2:-2] for x in list(re.finditer(r'\\*\\*.+?\\*\\*', r.split('# Changes')[1].split('# New Prompt')[0].replace('****', '** **')))]\n",
    "        rep_text = \"\"\n",
    "        if not len(replacements) % 2 == 0:\n",
    "            print(r)\n",
    "        while len(replacements) > 0:\n",
    "            r2 = replacements.pop(0)\n",
    "            r1 = replacements.pop(0)\n",
    "            if r2 == r1:\n",
    "                continue\n",
    "            rep_text += \" - \" + r1 + \" -> \" + r2 + \"\\n\" \n",
    "        return rep_text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def extract_prompt(r):\n",
    "    return r.split(\"# New Prompt:\\n\\n\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf4241-bea5-4a7c-883e-3d2f110a4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(prompt, verbose=False):\n",
    "    pseudonym_input = privacy_prompt.format(prompt=prompt)\n",
    "    pseudonym_output = privacy([\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': pseudonym_input,\n",
    "        }\n",
    "    ], max_new_tokens=1024)[0]['generated_text'][-1]['content']\n",
    "    \n",
    "    privatized_prompt = pseudonym_output.split(\"# New Prompt:\\n\\n\")[-1]\n",
    "    changes = extract_changes(pseudonym_output)\n",
    "    \n",
    "    if verbose:\n",
    "        print('[PSEUDONYMIZATION OUTPUT]')\n",
    "        print(pseudonym_output, '\\n\\n')\n",
    "    \n",
    "    model_response = chatGPT(privatized_prompt)\n",
    "\n",
    "    if verbose:\n",
    "        print('[INITIAL OUTPUT]')\n",
    "        print(model_response, '\\n\\n')\n",
    "\n",
    "    replacement_input = replacement_prompt + '\\n\\n' + \"# Entities to be replaced: \\n\\n\" + changes + \"\\n\\n# Text To Modify:\\n\\n\" + model_response\n",
    "    \n",
    "    final_output = replacement([\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': replacement_input,\n",
    "        },\n",
    "    ], max_new_tokens=2048)[0]['generated_text'][-1]['content']\n",
    "\n",
    "    if verbose:\n",
    "        print('[FINAL OUTPUT]')\n",
    "        print(final_output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9e5be-757f-4d78-9a18-098a087e17b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_privacy(inputs):\n",
    "    inputs = [privacy_prompt.format(prompt=i) for i in inputs]\n",
    "    inputs = [[{'role': 'user', 'content': i}] for i in inputs]\n",
    "    return privacy(inputs, max_new_tokens=1024)\n",
    "def run_replacement(privacy_output, model_responses):\n",
    "    privacy_changes = [extract_changes(x) for x in privacy_output]\n",
    "    replacement_inputs = [\n",
    "        replacement_prompt + '\\n\\n' + \"# Entities to be replaced: \\n\\n\" + changes + \"\\n\\n# Text To Modify:\\n\\n\" + model_response\n",
    "        for model_response, changes in  zip(model_responses, privacy_changes)\n",
    "    ]\n",
    "    replacement_inputs = [ [{'role': 'user', 'content': replacement_input}] for replacement_input in replacement_inputs]\n",
    "    return replacement(replacement_inputs, max_new_tokens=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0be5fa-83b5-44b4-95b4-760b4bb920c2",
   "metadata": {},
   "source": [
    "## Processing the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70290e3b-8ff4-49c8-b245-76dab89c44d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('prompts.json') as f:\n",
    "    test_data = json.load(f)\n",
    "test_data = test_data[int(len(test_data) * 0.75):]\n",
    "len(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d0985-c29e-4193-ac87-cf896b0e71b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_privacy = run_privacy(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cfc4e5-748b-4e79-ab2e-7a28ab217068",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_privacy_responses = [r[0]['generated_text'][-1]['content'] for r in test_data_privacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab45e6-d08a-4aed-ba8f-d9a7f88c8540",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_privacy_modprompts = [extract_prompt(r) for r in test_data_privacy_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4841c3c-7cae-44b5-88b0-499271f0a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed = []\n",
    "\n",
    "for i, prompt in enumerate(test_data_privacy_modprompts):\n",
    "    test_processed.append({\n",
    "        \"custom_id\": \"prompt\" + str(i),\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\"model\": \"gpt-4o-mini\", \"messages\": [{'role': 'user', 'content': prompt}], \"temperature\": 0.0}\n",
    "    })\n",
    "\n",
    "jsonl = '\\n'.join([json.dumps(line) for line in test_processed])\n",
    "with open('test_prompt_batch.jsonl', 'w') as f:\n",
    "    f.write(jsonl)\n",
    "\n",
    "batch_input_file = client.files.create(\n",
    "  file=open(\"test_prompt_batch.jsonl\", \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n",
    "\n",
    "batch_input_file_id = batch_input_file.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc182ce-035a-4154-afd3-55bf60d1382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"Test set responses.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a76070-ee12-4861-a69b-13bb30db58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_prompt_batch.jsonl') as f:\n",
    "    test_data_privacy = [json.loads(x)['body']['messages'][0]['content'] for x in f.read().split('\\n')[0:-1]]\n",
    "test_data_privacy[-1] # Privatized prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2bd3e6-aae0-4de2-af71-ebf92ee9bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_set_responses.jsonl') as f:\n",
    "    test_data_responses = [extract_reply(json.loads(r)) for r in f.read().split('\\n')[0:-1]]\n",
    "# Responses\n",
    "test_data_responses[-3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2a423-1b47-4a41-96de-eccc9f0a91ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_results = run_replacement(test_data_privacy, test_data_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b79623a-060f-411d-a8c0-ebb9285a1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_results_response = [r[0]['generated_text'][-1]['content'] for r in test_data_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b447a67-16cf-49fc-8b67-6ebb7a4d74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_data_results.json', 'w') as f:\n",
    "    json.dump(test_data_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cb60d-b955-4b35-adb3-b2d24d82c0b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('test_data_results.json') as f:\n",
    "    test_data_results_response = [x[0]['generated_text'][-1]['content'] for x in json.load(f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f2763c-56f8-4aac-8118-c10cce58968a",
   "metadata": {},
   "source": [
    "# Classifier Based Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838a541d-b703-4408-91a6-9fad27fc4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "   load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   labels = np.argmax(labels, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b29e0b-38cb-4687-b869-768ab52a3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = AutoModelForSequenceClassification.from_pretrained('google-bert/bert-base-uncased', num_labels=2)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d65e2-81a1-49c9-84c8-129017a761de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gold_replies.jsonl') as f:\n",
    "    gold_replies = [extract_reply(json.loads(r)) for r in f.read().split('\\n')[0:-1]]\n",
    "gold_test = gold_replies[int(len(gold_replies)*.75):]\n",
    "gold_test = [p + '\\n\\n' + g for p, g in zip(test_data, gold_test)]\n",
    "bert_test = [p + '\\n\\n' + g for p, g in zip(test_data, test_data_results_response)]\n",
    "len(gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df3f6b5-beee-4c6a-a437-c31e4547c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_gold = bert_tokenizer(gold_test, truncation=True, padding=\"max_length\")\n",
    "tokenized_test = bert_tokenizer(bert_test, truncation=True, padding=\"max_length\")\n",
    "\n",
    "bert_data = tokenized_gold['input_ids'] + tokenized_test['input_ids']\n",
    "bert_labels = ([[1.0, 0.0]]*len(tokenized_gold['input_ids'])) + ([[0, 1.0]]*len(tokenized_test['input_ids']))\n",
    "\n",
    "bert_data = list(zip(bert_data, bert_labels))\n",
    "random.shuffle(bert_data)\n",
    "bert_data, bert_labels = list(zip(*bert_data))\n",
    "\n",
    "\n",
    "bert_split = int(len(bert_data)*0.6)\n",
    "\n",
    "bert_train_data = Dataset.from_dict({\n",
    "    \"input_ids\": bert_data[:bert_split],\n",
    "    \"labels\": bert_labels[:bert_split]\n",
    "})\n",
    "bert_eval_data = Dataset.from_dict({\n",
    "    \"input_ids\": bert_data[bert_split:],\n",
    "    \"labels\": bert_labels[bert_split:]\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd90c2e-8a2f-40d7-aa7d-90ff8f4564b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "   './bertresults/',\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=8,\n",
    "   per_device_eval_batch_size=8,\n",
    "   num_train_epochs=5,\n",
    "   weight_decay=0.01,\n",
    "   eval_steps=10,\n",
    "   logging_steps=5,\n",
    "   eval_strategy=\"steps\",\n",
    "   do_eval=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "   model=bert,\n",
    "   args=training_args,\n",
    "   train_dataset=bert_train_data,\n",
    "   eval_dataset=bert_eval_data,\n",
    "   tokenizer=bert_tokenizer,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf6d5c-5327-4ada-81f4-db2645b4b97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4406ae8-2fdf-4ecb-9ba8-34f28867849a",
   "metadata": {},
   "source": [
    "# Human Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a169f-13eb-4607-83cf-5e25a086a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9758b-bb7e-4e66-870f-ff933f66cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use if  you don't need the saved data\n",
    "private_prompts = test_data_privacy_modprompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ddb83e-86d9-4837-bf84-e83c1a08ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_prompt_batch.jsonl') as f:\n",
    "    private_prompts = [json.loads(l)['body']['messages'][0]['content'] for l in f.read().split('\\n')[:-1]]\n",
    "with open('aggregate.json') as f:\n",
    "    human_eval = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71487b86-6d18-4fae-a9c0-800bdb2b935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(prompt, evaluation):\n",
    "    privacy_error = False\n",
    "    meaning_error = False\n",
    "    error = False\n",
    "    if evaluation['reject']:\n",
    "        return False, False, False\n",
    "    for tag, rel in zip(evaluation['tags'], evaluation['rel']):\n",
    "        if tag in prompt and not rel:\n",
    "            privacy_error = True\n",
    "            error = True\n",
    "        elif not tag in prompt and rel:\n",
    "            meaning_error = True\n",
    "            error = True\n",
    "    \n",
    "    return privacy_error, meaning_error, error\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbeba09-446c-4e5f-b0b1-adb9ff70bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_eval[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c075d-ac5b-47da-9e6c-6633febba9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [score(p, e) for p, e in zip(private_prompts, human_eval)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d0c54-9717-40b1-8701-a5d601759fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_scores, meaning_scores, errors = (zip(*scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8a41a8-33cc-4be0-8c89-fafe8677ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_eval_table = [\n",
    "    [\"Error Type\", \"# Errored Prompts\", \"% Errored Prompts\"],\n",
    "    [\"Privacy\", sum(privacy_scores), sum(privacy_scores)/len(privacy_scores)],\n",
    "    [\"Meaning\", sum(meaning_scores), sum(meaning_scores)/len(meaning_scores)],\n",
    "    [\"Errors\", sum(errors), sum(errors)/len(errors)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa741ef-29f7-40f2-86d4-2afca6027d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(human_eval_table[1:], columns=human_eval_table[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be4395-0729-49cf-a372-12eb0e0b87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score2(prompts, evaluations):\n",
    "    privacy_errored_tags = {}\n",
    "    meaning_errored_tags = {}\n",
    "    privacy_errors = 0\n",
    "    meaning_errors = 0\n",
    "    total = 0\n",
    "    total_meaning = 0\n",
    "    for prompt, evaluation in zip(prompts, evaluations):\n",
    "        if evaluation['reject']:\n",
    "            continue\n",
    "        for tag, rel, t in zip(evaluation['tags'], evaluation['rel'], evaluation['types']):\n",
    "            if tag in prompt and not rel:\n",
    "                privacy_errors += 1\n",
    "                privacy_errored_tags.setdefault(t, 0)\n",
    "                privacy_errored_tags[t] += 1\n",
    "            elif not tag in prompt and rel:\n",
    "                meaning_errors += 1\n",
    "                meaning_errored_tags.setdefault(t, 0)\n",
    "                meaning_errored_tags[t] += 1\n",
    "            total += 1\n",
    "    return privacy_errors / total, meaning_errors / total, privacy_errored_tags, meaning_errored_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e4c3a-b42f-47c1-bf98-058a64f5eff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score2(private_prompts, human_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58dec5b-0ede-46dc-8fe0-8b25ba0b1394",
   "metadata": {},
   "source": [
    "## ROUGE and BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baabef37-88f2-4440-aac3-e9e852cbe484",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gold_replies.jsonl') as f:\n",
    "    gold_replies = [extract_reply(json.loads(r)) for r in f.read().split('\\n')[0:-1]]\n",
    "gold_test = gold_replies[int(len(gold_replies)*.75):]\n",
    "gold_test = [g for p, g in zip(test_data, gold_test)]\n",
    "predicted_test = [g for p, g in zip(test_data, test_data_responses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d48e5a-30ae-4744-bc6a-e16c4dff1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b51a7c-5e74-4564-9c0e-f945a25fd47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f03cbc6-bf06-4212-b953-81463c227c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36674e8-3ddd-4f07-ae13-8b01013b2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7f3f8-323c-48f7-aa83-52eea5fbbbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores = [scorer.score(g, p) for g, p in zip(gold_test, predicted_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9148cfc-2be6-42af-b6fa-b2e01147edf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f7ac0-fcc8-422a-9ddf-cf9e8e7eee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores[0]['rouge1'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b7fc8-121c-4ab4-a90b-ba59eb65c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_f_scores = {\n",
    "    'rouge1': np.mean([x['rouge1'].fmeasure for x in rouge_scores]),\n",
    "    'rouge2': np.mean([x['rouge2'].fmeasure for x in rouge_scores]),\n",
    "    'rougeL': np.mean([x['rougeL'].fmeasure for x in rouge_scores]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b07c3ee-eff4-4112-947b-6ded11820af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_f_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95203d-3949-4fa6-82bd-c722a23a9024",
   "metadata": {},
   "source": [
    "## BLEU Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002c022-cbe9-4f7b-b534-1aa931e9507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d4a6b0-bb54-4be5-a8c6-95fe2aceab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [[x.split()] for x in gold_test]\n",
    "samples = [x.split() for x in bert_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b491fb4-e382-4c79-bd97-0b64677096bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cumulative 1-gram: %f' % np.mean([sentence_bleu(reference, sample, weights=(1, 0, 0, 0)) for reference, sample in zip(references, samples)]))\n",
    "print('Cumulative 2-gram: %f' % np.mean([sentence_bleu(reference, sample, weights=(0.5, 0.5, 0, 0))  for reference, sample in zip(references, samples)]))\n",
    "print('Cumulative 3-gram: %f' % np.mean([sentence_bleu(reference, sample, weights=(0.333, 0.333, 0.333, 0))  for reference, sample in zip(references, samples)]))\n",
    "print('Cumulative 4-gram: %f' % np.mean([sentence_bleu(reference, sample, weights=(0.25, 0.25, 0.25, 0.25))  for reference, sample in zip(references, samples)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
