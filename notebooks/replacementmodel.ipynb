{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e9903-908d-4f2c-a49d-dc4980822abd",
   "metadata": {
    "id": "b2ed5e96-5ca9-409d-be71-02b4bc764ce0"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, TrainingArguments, AutoTokenizer, Trainer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import huggingface_hub\n",
    "from datasets import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c0295-ba29-4af8-a77e-657204f7099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d634b-1fdf-4f18-9d99-0768053cc698",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = use_4bit,\n",
    "    bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    "    bnb_4bit_use_double_quant = use_nested_quant,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad330b-ec56-4364-8483-cf02079e5d4d",
   "metadata": {
    "id": "b17446e4-ddde-4704-ae13-f4e9ff7519ec"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/u/jas644/gemma-2-2b-it\", device_map='auto')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/u/jas644/gemma-2-2b-it\", device_map='auto')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c2294-71c3-49b2-ac8d-864f49e17530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reply(api_response):\n",
    "    return api_response['response']['body']['choices'][0]['message']['content']\n",
    "def restructure(r):\n",
    "    r = json.loads(r)\n",
    "    model_response = \"\"\"\n",
    "# Changes:\n",
    "\n",
    "\"\"\"\n",
    "    for change in r['changed_entities']:\n",
    "        model_response += \"## **\"  + change['original_entity'] + \"** changed to **\" + change['new_entity'] + \"**\\n\\n\" + change['explanation'] + \"\\n\\n\"\n",
    "    model_response += \"# New Prompt:\\n\\n\" + r['modified_prompt']\n",
    "\n",
    "    return model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a90a872-59b4-4ccf-826d-605ad9065b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are ReplaceGPT, an entity replacement model. Your task is to take an input, and output a transformed response that replaces all of the entities specified.\n",
    "\n",
    "The goal is to minimize the impact of changing the entities. The user should not be able to tell this transformation happened.\n",
    "\n",
    "The user will provide input of the original text, and a list of the entities that must be changed.\n",
    "\n",
    "You will output the modified text.\n",
    "\n",
    "Do not make any unnecesary changes that effect the semantic quality of the text, the meaning should stay the same.\n",
    "\n",
    "Only the entities themselves should change, not the meaning.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17224bcb-cac3-45d0-9cf6-0de1a1f81694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "def restructure_again(r):\n",
    "    try:\n",
    "        replacements =  [x.group()[2:-2] for x in list(re.finditer(r'\\*\\*.+?\\*\\*', r.split('# Changes')[1].split('# New Prompt')[0].replace('****', '** **')))]\n",
    "        rep_text = \"\"\n",
    "        if not len(replacements) % 2 == 0:\n",
    "            print(r)\n",
    "        while len(replacements) > 0:\n",
    "            r2 = replacements.pop(0)\n",
    "            r1 = replacements.pop(0)\n",
    "            if r2 == r1:\n",
    "                continue\n",
    "            rep_text += \" - \" + r1 + \" -> \" + r2 + \"\\n\" \n",
    "        return rep_text\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a74721-754f-423c-b3c3-b28e52092880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "\n",
    "# THIS CAN BE LOADED USING THE replacement_data.json IN THE GITHUB, as these files are not provided directly.\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "prompts = []\n",
    "with open('newmodifiedresponses.jsonl') as f:\n",
    "    for line in f.read().split('\\n')[:-1]:\n",
    "        r = json.loads(line)\n",
    "        prompts.append(\n",
    "            (extract_reply(r))\n",
    "        )\n",
    "\n",
    "responses = []\n",
    "with open('newstructuredresponses.jsonl') as f:\n",
    "    for line in f.read().split('\\n')[:-1]:\n",
    "        r = json.loads(line)\n",
    "        \n",
    "        responses.append(\n",
    "            restructure(extract_reply(r))\n",
    "        )\n",
    "\n",
    "replacement_gold = []\n",
    "with open('newreplacements.jsonl') as f:\n",
    "    for line in f.read().split('\\n')[:-1]:\n",
    "        r = json.loads(line)\n",
    "        try:\n",
    "            replacement_gold.append(json.loads(extract_reply(r))['modified_output'])\n",
    "        except:\n",
    "            print(r)\n",
    "\n",
    "        \n",
    "inputs = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\":\n",
    "            system_prompt + '\\n\\n' +\n",
    "            \"# Entities to be replaced: \\n\\n\" +\n",
    "            restructure_again(r) + \n",
    "            \"\\n\\n# Text To Modify:\\n\\n\" +\n",
    "            p\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": g\n",
    "        }\n",
    "    ]\n",
    "    for r, p, g in zip(responses, prompts, replacement_gold)\n",
    "]\n",
    "\n",
    "evals =  inputs[int(len(prompts) * 0.75):]\n",
    "inputs = inputs[:int(len(prompts) * 0.75)]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(inputs)\n",
    "inputs = tokenizer.batch_decode(inputs)\n",
    "dataset = Dataset.from_dict({\"input\": inputs, \"labels\": inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dccc3c0-6580-4a39-927d-82ec04a2a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "#number of training epochs\n",
    "num_train_epochs = 5\n",
    "\n",
    "#enable fp16/bf16 training (set bf16 to True when using A100 GPU in google colab)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "#batch size per GPU for training\n",
    "per_device_train_batch_size = 2\n",
    "\n",
    "#batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 2\n",
    "\n",
    "#gradient accumulation steps - No of update steps\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "#learning rate\n",
    "learning_rate = 5e-4\n",
    "\n",
    "#weight decay\n",
    "weight_decay = 0.001\n",
    "\n",
    "#Gradient clipping(max gradient Normal)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "#optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "#learning rate scheduler\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "#seed for reproducibility\n",
    "seed = 15132135\n",
    "\n",
    "#Number of training steps\n",
    "max_steps = -1\n",
    "\n",
    "#Ratio of steps for linear warmup\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "#group sequnces into batches with same length\n",
    "group_by_length = True\n",
    "\n",
    "#save checkpoint every X updates steps\n",
    "save_steps = 500\n",
    "\n",
    "#Log at every X updates steps\n",
    "logging_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5c144-c54f-4f01-b3cb-7dd1f167cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conv1D\n",
    "import torch\n",
    "\n",
    "def get_specific_layer_names(model):\n",
    "    # Create a list to store the layer names\n",
    "    layer_names = []\n",
    "    \n",
    "    # Recursively visit all modules and submodules\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the module is an instance of the specified layers\n",
    "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d, Conv1D)):\n",
    "            # model name parsing \n",
    "\n",
    "            layer_names.append('.'.join(name.split('.')[4:]).split('.')[0])\n",
    "    \n",
    "    return layer_names\n",
    "\n",
    "modules = list(set(get_specific_layer_names(model)))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aed589-6878-4668-94cf-373c2a3c0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 64 #lora attention dimension/ rank\n",
    "lora_alpha = 16 #lora scaling parameter\n",
    "lora_dropout = 0.1 #lora dropout probability\n",
    "\n",
    "#maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "packing = False\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha = lora_alpha,\n",
    "    lora_dropout = lora_dropout,\n",
    "    r  = lora_r,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5347b03b-4768-40c5-9988-4033c916a9eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set Training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "    optim = optim,\n",
    "    save_steps = save_steps,\n",
    "    logging_steps = logging_steps,\n",
    "    learning_rate = learning_rate,\n",
    "    fp16 = fp16,\n",
    "    bf16 = bf16,\n",
    "    # remove_unused_columns=False,\n",
    "    max_grad_norm = max_grad_norm,\n",
    "    weight_decay = weight_decay,\n",
    "    lr_scheduler_type = lr_scheduler_type,\n",
    "    warmup_ratio = warmup_ratio,\n",
    "    group_by_length = group_by_length,\n",
    "    max_steps = max_steps,\n",
    "\n",
    ")\n",
    "\n",
    "#SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    peft_config = peft_config,\n",
    "    dataset_text_field = \"input\",\n",
    "    max_seq_length = 2512,\n",
    "    args = training_arguments,\n",
    "    tokenizer=tokenizer,\n",
    "    packing = packing,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
