{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2192ed84-7ee4-4c8e-b549-7c1648822310",
   "metadata": {
    "id": "b2ed5e96-5ca9-409d-be71-02b4bc764ce0"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, TrainingArguments, AutoTokenizer, AutoModelForCausalLM, Trainer\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import huggingface_hub\n",
    "from datasets import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c78fe4-ba44-400c-b339-07a9379619b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e362637-2bd4-4d31-83bc-8400f1dea71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = use_4bit,\n",
    "    bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    "    bnb_4bit_use_double_quant = use_nested_quant,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17446e4-ddde-4704-ae13-f4e9ff7519ec",
   "metadata": {
    "id": "b17446e4-ddde-4704-ae13-f4e9ff7519ec"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/u/jas644/gemma-2-2b-it\", device_map='auto')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/u/jas644/gemma-2-2b-it\", device_map='auto')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0924c-27c1-4290-8bd9-78229c6bd855",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_gemma = \"\"\"\n",
    "You are PrivacyGPT. You will anonymize the user's prompt while maintaining the meaning whenever possible.\n",
    "\n",
    "Your task is to revise the user's prompt. Your goal is to reword and change all private entities that are not strictly relevant to the text. You can change any names, places, organizations, etc as long as they wont effect the response when changed back.\n",
    "\n",
    "Remember the following criteria:\n",
    "* The meaning of the prompt **must not change**.\n",
    "* Any replacement of names, places, organizations, etc must be semantically similar to their original values. \n",
    "    * Don't use placeholders like [NAME]. Opt for similar entities, such as names with the same gender, organizations in the same field, etc.\n",
    "    * We will replace these replacements again when their response is completed, so the user does not notice the effect.\n",
    "* If the private entities are crucial to the meaning of the prompt then they must stay as they appear.\n",
    "    * For example, a location may remain in the prompt if it is absolutely needed to create a response and a replacement would not work.\n",
    "    * Works of literature often do not rely on the entity remaining the same, but there are exceptions, for example if the user requests a rhyming poem or song.\n",
    "* You are **maximizing the privacy** of the user, and **minimizing the effect on their request's reponse**.\n",
    "\n",
    "You will return your reasoning for each change alongside the change itself. At the end, provide the fully modified prompt as well as the original prompt.\n",
    "\n",
    "**REMEMBER: ONLY REPLACE THE WORD/TOKEN IF IT WILL NOT CHANGE THE ANSWER OR RESPONSE OF THE QUESTION OR TASK.**\n",
    "\n",
    "Here is the prompt:\n",
    "\n",
    "{prompt}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2334c-a23e-4e7c-8a53-0ab57ce36c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reply(api_response):\n",
    "    return api_response['response']['body']['choices'][0]['message']['content']\n",
    "\n",
    "def restructure(r):\n",
    "    r = json.loads(r)\n",
    "    model_response = \"\"\"\n",
    "# Changes:\n",
    "\n",
    "\"\"\"\n",
    "    for change in r['changed_entities']:\n",
    "        model_response += \"## **\"  + change['original_entity'] + \"** changed to **\" + change['new_entity'] + \"**\\n\\n\" + change['explanation'] + \"\\n\\n\"\n",
    "    model_response += \"# New Prompt:\\n\\n\" + r['modified_prompt']\n",
    "\n",
    "    return model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d5d3f-6ff8-4e23-89b5-695a4c36790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "# THIS CAN BE LOADED USING THE privacy_data.json IN THE GITHUB, as these files are not provided directly.\n",
    "\n",
    "import json\n",
    "with open('prompts.json') as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "responses = []\n",
    "with open('newstructuredresponses.jsonl') as f:\n",
    "    for line in f.read().split('\\n')[:-1]:\n",
    "        r = json.loads(line)\n",
    "        responses.append(\n",
    "            extract_reply(r)\n",
    "        )\n",
    "prompts = [\n",
    "    [\n",
    "        # system_prompt,\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": system_prompt_gemma.format(prompt=p)\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": restructure(r)\n",
    "        }\n",
    "        \n",
    "    ]\n",
    "    for p, r in zip(prompts, responses)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "evals =  prompts[int(len(prompts) * 0.75):]\n",
    "prompts = prompts[:int(len(prompts) * 0.75)]\n",
    "\n",
    "inputs = [[{\"role\": \"user\", \"content\": system_prompt.format(prompt=p.replace('\\n', ' ').replace('\\\\', '\\\\\\\\'))}] for p in prompts]\n",
    "inputs = tokenizer.apply_chat_template(inputs)\n",
    "inputs = tokenizer.batch_decode(inputs)\n",
    "dataset = Dataset.from_dict({\"input\": inputs, \"labels\": inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabdd677-c5a9-4e14-96bf-04baac1ce4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "#number of training epochs\n",
    "num_train_epochs = 5\n",
    "\n",
    "#enable fp16/bf16 training (set bf16 to True when using A100 GPU in google colab)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "#batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "#batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "#gradient accumulation steps - No of update steps\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "#learning rate\n",
    "learning_rate = 5e-4\n",
    "\n",
    "#weight decay\n",
    "weight_decay = 0.001\n",
    "\n",
    "#Gradient clipping(max gradient Normal)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "#optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "#learning rate scheduler\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "#seed for reproducibility\n",
    "seed = 15132135\n",
    "\n",
    "#Number of training steps\n",
    "max_steps = -1\n",
    "\n",
    "#Ratio of steps for linear warmup\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "#group sequnces into batches with same length\n",
    "group_by_length = True\n",
    "\n",
    "#save checkpoint every X updates steps\n",
    "save_steps = 500\n",
    "\n",
    "#Log at every X updates steps\n",
    "logging_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5333b0-82a9-49f4-8e31-879360536042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conv1D\n",
    "import torch\n",
    "\n",
    "def get_specific_layer_names(model):\n",
    "    # Create a list to store the layer names\n",
    "    layer_names = []\n",
    "    \n",
    "    # Recursively visit all modules and submodules\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the module is an instance of the specified layers\n",
    "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d, Conv1D)):\n",
    "            # model name parsing \n",
    "\n",
    "            layer_names.append('.'.join(name.split('.')[4:]).split('.')[0])\n",
    "    \n",
    "    return layer_names\n",
    "\n",
    "modules = list(set(get_specific_layer_names(model)))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b66b4-2b44-4739-ad21-2576d4334688",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 64 #lora attention dimension/ rank\n",
    "lora_alpha = 16 #lora scaling parameter\n",
    "lora_dropout = 0.1 #lora dropout probability\n",
    "\n",
    "max_seq_length = None\n",
    "\n",
    "packing = False\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha = lora_alpha,\n",
    "    lora_dropout = lora_dropout,\n",
    "    r  = lora_r,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace53d9-590d-4395-8a39-ed888264e46e",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set Training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "    optim = optim,\n",
    "    save_steps = save_steps,\n",
    "    logging_steps = logging_steps,\n",
    "    learning_rate = learning_rate,\n",
    "    fp16 = fp16,\n",
    "    bf16 = bf16,\n",
    "    # remove_unused_columns=False,\n",
    "    max_grad_norm = max_grad_norm,\n",
    "    weight_decay = weight_decay,\n",
    "    lr_scheduler_type = lr_scheduler_type,\n",
    "    warmup_ratio = warmup_ratio,\n",
    "    group_by_length = group_by_length,\n",
    "    max_steps = max_steps,\n",
    "\n",
    ")\n",
    "\n",
    "#SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    peft_config = peft_config,\n",
    "    dataset_text_field = \"input\",\n",
    "    max_seq_length = 1024,\n",
    "    args = training_arguments,\n",
    "    tokenizer=tokenizer,\n",
    "    packing = packing,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
